## SGD优化
- 实现常见的SGD优化算法
    - sgd
    - Momentum
    - Nesterov 加速
    - Adagrad
    - Adam
    - RMSprop
    - Adadelta
    - adabound
- 将实现的算法应用到一个简单的逻辑回归任务上面,比较他们的效果
- 做好实验报告


## 思考题
- sgd收敛条件对学习率有什么要求? 上述这些算法是否都满足这个要求?
- Momentum的几何意义是什么?
- Nesterov加速如何理解?为什么可以加速
- adabound为什么可以解决Adam的收敛问题? 请举例说明Adam不收敛