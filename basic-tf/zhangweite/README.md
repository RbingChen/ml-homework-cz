## 实验报告
## 实验原理
1.利用TensorFlow搭建MLP（LR）、CNN和RNN网络，实现MNIST数据集的分类。
2.利用summary与tensorboard记录权值与结果，根据结果调整网络超参数。
## 主要步骤
1.获取MNIST数据集，并利用placeholder或dataset方式输入网络。
2.MLP相当于在LR的基础上添加了hidden layer。
3.基本的CNN网络由多个卷积层、pooling层和flatten层组成，利用stride=2的卷积层代替max_pooling层会有更好的效果，max_pooling相当于“多段线性的激活函数”。
4.RNN的结构采用lstm，同理可扩展为双向RNN和多层RNN。
5.网络层参数维度较大的情况添加dropout层。
6.batch_normalization层在batch数量较小（50～100）的时候效果并不是很好。
7.网络权值随机初始化能够加速网络收敛，甚至能和激活函数搭配使用（selu）。
8.test准确率不能直接调用tf.metrics.accuracy，它指的是整个session内，所有feed_dict数据的正确率。
## 实验结果
batch_size=100的情况下，
-MLP训练10000个batch，test acc=0.9398
-CNN训练1000个batch，test acc=0.9832
-RNN训练10000个batch，test acc=0.9804
上述三个网络均采用了简单的网络结构（电脑跑不动- -），并且在训练中没有收敛。
总结来说，CNN网络能够较好的对图片做分类，但RNN能够在参数量较少的情况下达到较高的准确率令人惊讶。

## 思考题
1. 请使用自己的语言举例说明TensorFlow的声明式编程和我们日常的编程方式上的区别? 如何理解TensorFlow中一切都是计算图上的节点? TensorFlow中写日志操作也是计算图中的节点吗?
-TF的声明式编程相当于提前建立好计算框架，包括计算节点以及数据维度，在搭建框架时并不进行计算。在执行的时候给予输入以及执行命令，整个计算框架才开始计算。而日常的编程则是顺序执行的，一旦执行完语句之后就会得到结果。TF这种运行模式适合于机器学习训练以及预测的模式。
-TF中的计算图包括节点与连接线，节点表示数据输入/输出、权值以及计算模块，连接线则表示了数据的传递方向，所以TensorFlow计算图上的节点基本反应了整个计算框架。
-写日志操作只是记录节点输出，应该不算计算图中的节点。

2. 下面的代码有区别吗?请详细说明。 其中预定义 `W = tf.Variable(tf.zeros(10))`, `g = np.random.rand(10)`
    - `W = W - 0.1 * g`
    - `W.assign_sub(0.1 * g)`
第一行代码计算后更新了W，但第二行没有更新W，只是返回了结果。

3. 请说明张量、操作、变量之间的区别和联系
-张量是TF中对运算结果的引用，他保存的是得到该结果的计算过程，包括名字、维度和类型三个属性。
-变量包含张量，具有内存缓冲。变量需初始化，并在训练时进行数据读写。
-操作（我理解的Session）是指执行某一部分计算图的过程，计算过程中包含诸多的张量与变量。

4. 请使用tensorboard查看计算图可视化的结果, 思考: 为什么TensorFlow要建立一张反向计算图, 而不是直接在原图中实现BP算法?
反向图从深层往浅层计算梯度，计算逻辑顺畅。

5. tensorboard的直方图怎么查看? 怎么将训练集和测试集的数据显示在同一张图上?


6. 从checkpoint中恢复模型的原理是什么?是如何将保存的值和张量关联上的?
变量读取之前训练的数据，恢复训练模型。通过对应表关联保存的值和张量。

7. 使用变量作用域的好处是啥?
更方便的管理参数名，比如两个作用域中的命名可以重复。

8. 分布式训练的时候, 程序是如何组织server端代码和worker端代码, 使得两部分代码可以放到一个脚本文件中的?
用if-elif来组织的。

9. 用自己的话简述分布式训练时Server和Worker分别做了哪些事情? 参数服务器与MapReduce的区别有哪些?
-Server负责更新参数，Worker负责计算梯度。
-参数服务器相较于MapReduce的优点：参数全局共享，支持异步通信，新增节点不需要重启。

10. 分布式训练的时候,同步更新和异步更新的区别和优缺点,TensorFlow在异步更新的时候如何保证server端数据的一致性?
-同步更新是指全部worker完成后再统一更新参数，优点是参数更新稳定，缺点是有木桶效应。
异步更新是指任一worker完成后直接更新server的参数（更新方式：1.利用其他worder的历史数据来平均，2.单独利用该worker的结果更新，3.取一个权衡），优点是效率高，缺点是参数更新不稳定。
-TensorFlow在异步更新的时候利用一致性hash算法来保证server端数据一致性，算法主要原理是将数据和server利用hash算法映射到圆环上，并将数据存储到顺时针最近的server上。一致性hash算法也使得server增删时，失效的缓存数量最少。
 

11. `placeholder`方式输入数据和`dataset`方式的区别和利弊有哪些?为什么实际使用中更倾向于使用`dataset`API?
Placeholder方式只能读内存数据，并需要提前确定数据维度。
Dataset同时支持从内存和硬盘里读取数据，并根据第一个维度利用iterator迭代读取，具有repeat、map、shuffle、batch等变换功能，更易于实际使用。
