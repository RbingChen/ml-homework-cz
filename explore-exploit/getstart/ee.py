#coding:utf-8

import numpy as np
import matplotlib.pyplot as plt
from numba import jit
import logging

"""
多臂老虎机问题: 有N个臂,每个臂都有概率pi有奖励,而1-pi的概率没有奖励。每一次都可以摇一个臂,显然最优策略是不停地摇pi最大的那个臂获得的期望奖励最大。但是问题是,pi是未知的,所以需要不断探索。本次作业实现几种不同的探索算法。
"""
n = 10 # 10个臂
bandit_p = np.random.rand(n)

def gen_data(i):
    assert i < n
    if np.random.rand() < bandit_p[i]:
        return 1.0  # reward
    return 0.0 # no reward

# 随机选择一个

def random_select(step, **kwargs):
    """
    :param step: 第step步
    :return: int
    """
    # TODO 随机从n个臂中选择一个
    raise NotImplementedError()



def determine_select(step, **kwargs):
    """

    :param step: 第step步
    :return: int
    """
    # TODO 前n步随机选, 从第n步开始,选择奖励最大的那个臂
    if step < n:
        raise NotImplementedError()
    else:
        raise NotImplementedError()

# epsilon 贪心

def epsilon_greedy(step, **kwargs):
    """

    :param step: 第step步
    :return: int
    """
    # TODO 以epsilon的概率用随机策略, 1-epsilon的概率用确定性策略, epsilon随着step增大而减小
    raise NotImplementedError()

# naive

def naive_select(step, **kwargs):
    """

    :param step: 第step步
    :param kwargs:
    :return: int
    """
    # TODO 前K步用随机策略,从第K+1步开始用确定性策略, K是超参数,自己调优
    raise NotImplementedError()

# softmax
def softmax_select(step, cum_reward_action, cum_action, **kwargs):
    """

    :param step: int 第step步
    :param cum_reward_action: np.array(int) 每个动作的累积回报, index是动作也就是臂的编号
    :param cum_action: np.array(int) 每个臂累积动作次数, index是动作,value是该臂被选择的次数
    :param kwargs:
    :return: int
    """

    # TODO 计算每个动作的平均回报 ci, 选择该动作的概率正比于  exp(ci/c) c是一个超参数
    raise NotImplementedError()

def thompson_select(step, cum_reward_action, cum_action, **kwargs):
    """

    :param step: int 第step步
    :param cum_reward_action: np.array(int) 每个动作的累积回报, index是动作也就是臂的编号
    :param cum_action: np.array(int) 每个臂累积动作次数, index是动作,value是该臂被选择的次数
    :param kwargs:
    :return: int
    """
    # TODO 每个臂的期望概率 p_i = (1 + win[i]) / (1 + trials[i]), 现在不是用期望的概率,
    # 而是引入一定的随机性,随机从p_i服从的后延分布beta分布中采样一个结果作为p_i的估计值,
    # 然后排序选出最大概率的臂
    # beta分布可以通过 np.random.beta(alpha, beta)生成

    raise NotImplementedError()

def ucb_select(step, cum_reward_action, cum_action, **kwargs):
    """

    :param step: int 第step步
    :param cum_reward_action: np.array(int) 每个动作的累积回报, index是动作也就是臂的编号
    :param cum_action: np.array(int) 每个臂累积动作次数, index是动作,value是该臂被选择的次数
    :param kwargs:
    :return: int
    """

    # TODO 用置信区间上界 \hat{r} + \sqrt{\frac{2 \ln T}{n_i}} 作为p_i的乐观估计值,
    # 然后排序选出最优动作。其中 T 是总的步数, n_i 是第i个臂被选择的次数。

    raise NotImplementedError()

@jit
def simulation(select_action, n_ = 3000):
    t = []
    reward = []
    cum_reward_per = 0.0
    cum_reward_action = np.zeros(n)
    cum_action = np.zeros(n)
    for i in range(n_):
        t.append(i)
        ctx = {'n' : n, 'cum_reward_action' : cum_reward_action, 'cum_action' : cum_action, 'step': i}
        action = select_action(**ctx)

        #TODO 根据结果更新
        # - cum_reward_per 本次仿真累积获得的奖励
        # - cum_reward_action, 每个动作累积获得的奖励
        # - cum_action 每个动作累积选择的次数
        raise NotImplementedError()

        reward.append(cum_reward_per)
    return np.array(t), np.array(reward)

@jit
def run(select_action, rnd_ = 100, n_ = 3000):
    t = []
    avg_reward = 0
    for i in range(rnd_):
        #TODO 仿真rnd_次,计算平均回报
        # 注意, avg_reward 是一个数组, 将每次 simulation 返回的
        # reward 数组加起来然后除以 rnd_
        raise NotImplementedError()


    return t, np.max(bandit_p) - avg_reward /(1 + np.arange(n_))


def main():
    n_ = 2000
    t, r = run(random_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(epsilon_greedy, n_ = n_)
    plt.plot(t, r)

    t, r = run(naive_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(thompson_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(ucb_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(softmax_select, n_ = n_)
    plt.plot(t, r)


    plt.legend(['random', 'epsilon greedy', 'naive', 'thompson select', 'ucb select', 'softmax select'])
    plt.xlabel('t')
    plt.ylabel('regret')
    plt.show()

if __name__ == '__main__':
    main()