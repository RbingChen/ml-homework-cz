#coding:utf-8

import numpy as np
import matplotlib.pyplot as plt
from numba import jit
import logging

"""
多臂老虎机问题: 有N个臂,每个臂都有概率pi有奖励,而1-pi的概率没有奖励。每一次都可以摇一个臂,显然最优策略是不停地摇pi最大的那个臂获得的期望奖励最大。但是问题是,pi是未知的,所以需要不断探索。本次作业实现几种不同的探索算法。
"""
n = 10 # 10个臂
bandit_p = np.random.rand(n)

def gen_data(i):
    assert i < n
    if np.random.rand() < bandit_p[i]:
        return 1.0  # reward
    return 0.0 # no reward

# 随机选择一个

def random_select(kwargs):
    """
    :param step: 第step步
    :return: int
    """
    # TODO 随机从n个臂中选择一个
    #raise NotImplementedError()
    bandit_n=np.random.randint(0,kwargs['n'])
    return bandit_n



def determine_select(kwargs):
    """

    :param step: 第step步
    :return: int
    """
    # TODO 前n步随机选, 从第n步开始,选择奖励最大的那个臂
    n=kwargs['n']
    if kwargs['step'] < n:
        #raise NotImplementedError()
        return np.random.randint(0,kwargs['n'])
    else:
        avg_reward=kwargs['cum_reward_action'] / (kwargs['cum_action']+0.1)
        return np.argmax(avg_reward)

# epsilon 贪心

def epsilon_greedy(kwargs):
    """

    :param step: 第step步
    :return: int
    """
    # TODO 以epsilon的概率用随机策略, 1-epsilon的概率用确定性策略, epsilon随着step增大而减小
    #raise NotImplementedError()
    epsilon=1.0/np.sqrt(kwargs['step']+1)
    if np.random.rand()<epsilon:
        return  np.random.randint(0,kwargs['n'])
    else:
        avg_reward=kwargs['cum_reward_action'] / (kwargs['cum_action']+0.1)
        return np.argmax(avg_reward)

# naive

def naive_select(kwargs):
    """

    :param step: 第step步
    :param kwargs:
    :return: int
    """
    # TODO 前K步用随机策略,从第K+1步开始用确定性策略, K是超参数,自己调优
    #raise NotImplementedError()
    if kwargs['step']<200:
       return np.random.randint(0,kwargs['n'])
    else:
       avg_reward=kwargs['cum_reward_action'] / (kwargs['cum_action']+0.1)
       return np.argmax(avg_reward)


# softmax
def softmax_select(kwargs):
    """

    :param step: int 第step步
    :param cum_reward_action: np.array(int) 每个动作的累积回报, index是动作也就是臂的编号
    :param cum_action: np.array(int) 每个臂累积动作次数, index是动作,value是该臂被选择的次数
    :param kwargs:
    :return: int
    """

    # TODO 计算每个动作的平均回报 ci, 选择该动作的概率正比于  exp(ci/c) c是一个超参数

    #raise NotImplementedError()
    avg_reward=kwargs['cum_reward_action'] / (kwargs['cum_action']+0.1)
    tau=0.1
    exp_p=np.exp(avg_reward/tau)
    p=exp_p/np.sum(exp_p)

    prob=np.random.rand()#简化版的轮盘赌输法
    for i in  xrange(kwargs['n']):
        prob-=p[i]
        if prob<0:
           return i
    return np.random.randint(0,kwargs['n'])




def thompson_select(kwargs):
    """

    :param step: int 第step步
    :param cum_reward_action: np.array(int) 每个动作的累积回报, index是动作也就是臂的编号
    :param cum_action: np.array(int) 每个臂累积动作次数, index是动作,value是该臂被选择的次数
    :param kwargs:
    :return: int
    """
    # TODO 每个臂的期望概率 p_i = (1 + win[i]) / (1 + trials[i]), 现在不是用期望的概率,
    # 而是引入一定的随机性,随机从p_i服从的后延分布beta分布中采样一个结果作为p_i的估计值,
    # 然后排序选出最大概率的臂
    # beta分布可以通过 np.random.beta(alpha, beta)生成

    win=kwargs['cum_action_have_reward']
    lose=kwargs['cum_action']-win
    p=np.zeros(kwargs['n'])
    for i in xrange(kwargs['n']):
        p[i]=np.random.beta(win[i]+1,lose[i]+1)
    return np.argmax(p)


def ucb_select(kwargs):
    """

    :param step: int 第step步
    :param cum_reward_action: np.array(int) 每个动作的累积回报, index是动作也就是臂的编号
    :param cum_action: np.array(int) 每个臂累积动作次数, index是动作,value是该臂被选择的次数
    :param kwargs:
    :return: int
    """

    # TODO 用置信区间上界 \hat{r} + \sqrt{\frac{2 \ln T}{n_i}} 作为p_i的乐观估计值,
    # 然后排序选出最优动作。其中 T 是总的步数, n_i 是第i个臂被选择的次数。
    cum_reward_avg=kwargs['cum_reward_action']/(kwargs['cum_action']+0.1)

    ucb=cum_reward_avg+np.sqrt(2*np.log(kwargs['step']+1)/(kwargs['cum_action']+0.1))
    return np.argmax(ucb)

@jit
def simulation(select_action, n_ = 3000):
    t = []
    reward = []
    cum_reward_per = 0.0
    cum_reward_action = np.zeros(n)#
    cum_action = np.zeros(n)
    cum_action_have_reward=np.zeros(n)
    for i in range(n_):
        t.append(i)
        ctx = {'n' : n, 'cum_reward_action' : cum_reward_action, 'cum_action' : cum_action,\
               'step': i,'cum_action_have_reward':cum_action_have_reward}
        action = select_action(ctx)

        #TODO 根据结果更新
        # - cum_reward_per 本次仿真累积获得的奖励
        # - cum_reward_action, 每个动作累积获得的奖励
        # - cum_action 每个动作累积选择的次数

        r=gen_data(action)
        cum_reward_action[action]+=r
        cum_reward_per+=r
        cum_action[action]+=1

        cum_action_have_reward[action]+=(1 if r>0.0 else 0)# 每个动作累积有奖赏的次数

        reward.append(cum_reward_per)
    return np.array(t), np.array(reward)

@jit
def run(select_action, rnd_ = 100, n_ = 3000):
    t = []
    avg_reward = 0
    for i in range(rnd_):
        #TODO 仿真rnd_次,计算平均回报
        # 注意, avg_reward 是一个数组, 将每次 simulation 返回的
        # reward 数组加起来然后除以 rnd_
        t, reward = simulation(select_action, n_)

        avg_reward = avg_reward+(reward-avg_reward)/(i+1)

        #raise NotImplementedError()


    return t, np.max(bandit_p) - avg_reward /(1 + np.arange(n_))


def main():
    n_ = 3000
    t, r = run(random_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(epsilon_greedy, n_ = n_)
    plt.plot(t, r)

    t, r = run(naive_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(thompson_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(ucb_select, n_ = n_)
    plt.plot(t, r)

    t, r = run(softmax_select, n_ = n_)
    plt.plot(t, r)

    plt.legend(['random', 'epsilon greedy', 'naive', 'thompson select', 'ucb select', 'softmax select'])
    plt.xlabel('t')
    plt.ylabel('regret')
    plt.show()

if __name__ == '__main__':
    main()