## 基本梯度下降优化
- 优化 basic-lr 中的逻辑回归, 考虑 L2 正则和 L1 正则
- 实现基本梯度梯度下降优化
- 实现牛顿法和拟牛顿法的二阶方法[OPTION]
- 实现L1正则的软阈值算法
- 实现L1正则的FTRL算法
- 参考
    - 近似梯度法 <https://tracholar.github.io/wiki/machine-learning/optimization/proximal-algorithm.html>
    - FTRL算法 Follow-the-Regularized-Leader and Mirror Descent: Equivalence Theorems and L1 Regularization
- 问题
    - 梯度下降常数学习率能收敛吗?
    - FTRL与软阈值算法的区别,分别试用的场景是啥?